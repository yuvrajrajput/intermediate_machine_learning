{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"By encoding **categorical variables**, you'll obtain your best results thus far!\n\n# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system.","metadata":{}},{"cell_type":"code","source":"# Set up code checking\nimport os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex3 import *\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:24.123814Z","iopub.execute_input":"2021-12-26T08:49:24.124226Z","iopub.status.idle":"2021-12-26T08:49:24.137055Z","shell.execute_reply.started":"2021-12-26T08:49:24.124197Z","shell.execute_reply":"2021-12-26T08:49:24.135884Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"In this exercise, you will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id') \nX_test = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:24.138871Z","iopub.execute_input":"2021-12-26T08:49:24.139403Z","iopub.status.idle":"2021-12-26T08:49:24.243832Z","shell.execute_reply.started":"2021-12-26T08:49:24.139357Z","shell.execute_reply":"2021-12-26T08:49:24.242415Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Use the next code cell to print the first five rows of the data.","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:24.245619Z","iopub.execute_input":"2021-12-26T08:49:24.245952Z","iopub.status.idle":"2021-12-26T08:49:24.271896Z","shell.execute_reply.started":"2021-12-26T08:49:24.245908Z","shell.execute_reply":"2021-12-26T08:49:24.271062Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Notice that the dataset contains both numerical and categorical variables.  You'll need to encode the categorical data before training a model.\n\nTo compare different models, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:24.273405Z","iopub.execute_input":"2021-12-26T08:49:24.273658Z","iopub.status.idle":"2021-12-26T08:49:24.286163Z","shell.execute_reply.started":"2021-12-26T08:49:24.273627Z","shell.execute_reply":"2021-12-26T08:49:24.285313Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Drop columns with categorical data\n\nYou'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively.  ","metadata":{}},{"cell_type":"code","source":"# Fill in the lines below: drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:24.288827Z","iopub.execute_input":"2021-12-26T08:49:24.289125Z","iopub.status.idle":"2021-12-26T08:49:24.303879Z","shell.execute_reply.started":"2021-12-26T08:49:24.289067Z","shell.execute_reply":"2021-12-26T08:49:24.302882Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach.","metadata":{}},{"cell_type":"code","source":"print(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:24.305683Z","iopub.execute_input":"2021-12-26T08:49:24.305922Z","iopub.status.idle":"2021-12-26T08:49:25.537721Z","shell.execute_reply.started":"2021-12-26T08:49:24.305894Z","shell.execute_reply":"2021-12-26T08:49:25.536954Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Before jumping into ordinal encoding, we'll investigate the dataset.  Specifically, we'll look at the `'Condition2'` column.  The code cell below prints the unique entries in both the training and validation sets.","metadata":{}},{"cell_type":"code","source":"print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\nprint(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:25.538767Z","iopub.execute_input":"2021-12-26T08:49:25.538988Z","iopub.status.idle":"2021-12-26T08:49:25.548316Z","shell.execute_reply.started":"2021-12-26T08:49:25.538960Z","shell.execute_reply":"2021-12-26T08:49:25.547368Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue.  For instance, you can write a custom ordinal encoder to deal with new categories.  The simplest approach, however, is to drop the problematic categorical columns.  \n\nRun the code cell below to save the problematic columns to a Python list `bad_label_cols`.  Likewise, columns that can be safely ordinal encoded are stored in `good_label_cols`.","metadata":{}},{"cell_type":"code","source":"# Categorical columns in the training data\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:25.549626Z","iopub.execute_input":"2021-12-26T08:49:25.549960Z","iopub.status.idle":"2021-12-26T08:49:25.567521Z","shell.execute_reply.started":"2021-12-26T08:49:25.549928Z","shell.execute_reply":"2021-12-26T08:49:25.566445Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Part B\n\nUse the next code cell to ordinal encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `label_X_train` and `label_X_valid`, respectively.  \n- We have provided code below to drop the categorical columns in `bad_label_cols` from the dataset. \n- You should ordinal encode the categorical columns in `good_label_cols`.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\nlabel_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\nlabel_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:25.568637Z","iopub.execute_input":"2021-12-26T08:49:25.568927Z","iopub.status.idle":"2021-12-26T08:49:25.620225Z","shell.execute_reply.started":"2021-12-26T08:49:25.568886Z","shell.execute_reply":"2021-12-26T08:49:25.619524Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach.","metadata":{}},{"cell_type":"code","source":"print(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:25.621544Z","iopub.execute_input":"2021-12-26T08:49:25.622374Z","iopub.status.idle":"2021-12-26T08:49:27.183242Z","shell.execute_reply.started":"2021-12-26T08:49:25.622330Z","shell.execute_reply":"2021-12-26T08:49:27.182430Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"So far, you've tried two different approaches to dealing with categorical variables.  And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n\nSoon, you'll try one-hot encoding.  Before then, there's one additional topic we need to cover.  Begin by running the next code cell without changes.  ","metadata":{}},{"cell_type":"code","source":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:27.184670Z","iopub.execute_input":"2021-12-26T08:49:27.185146Z","iopub.status.idle":"2021-12-26T08:49:27.200209Z","shell.execute_reply.started":"2021-12-26T08:49:27.185099Z","shell.execute_reply":"2021-12-26T08:49:27.199333Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Investigating cardinality\n\n### Part A\n\nThe output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n\nWe refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.\n\nUse the output above to answer the questions below.","metadata":{}},{"cell_type":"code","source":"# Fill in the line below: How many categorical variables in the training data\n# have cardinality greater than 10?\nhigh_cardinality_numcols = 3\n\n# Fill in the line below: How many columns are needed to one-hot encode the \n# 'Neighborhood' variable in the training data?\nnum_cols_neighborhood = 25\n","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:27.201251Z","iopub.execute_input":"2021-12-26T08:49:27.201618Z","iopub.status.idle":"2021-12-26T08:49:27.204748Z","shell.execute_reply.started":"2021-12-26T08:49:27.201590Z","shell.execute_reply":"2021-12-26T08:49:27.203993Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Part B\n\nFor large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.\n\nAs an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n- If we instead replace the column with the ordinal encoding, how many entries are added?  \n\nUse your answers to fill in the lines below.","metadata":{}},{"cell_type":"code","source":"# Fill in the line below: How many entries are added to the dataset by \n# replacing the column with a one-hot encoding?\nOH_entries_added = 1e4*100 - 1e4\n\n# Fill in the line below: How many entries are added to the dataset by\n# replacing the column with an ordinal encoding?\nlabel_entries_added = 0\n","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:27.206000Z","iopub.execute_input":"2021-12-26T08:49:27.206493Z","iopub.status.idle":"2021-12-26T08:49:27.217234Z","shell.execute_reply.started":"2021-12-26T08:49:27.206450Z","shell.execute_reply":"2021-12-26T08:49:27.216433Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Next, you'll experiment with one-hot encoding.  But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n\nRun the code cell below without changes to set `low_cardinality_cols` to a Python list containing the columns that will be one-hot encoded.  Likewise, `high_cardinality_cols` contains a list of categorical columns that will be dropped from the dataset.","metadata":{}},{"cell_type":"code","source":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:27.219693Z","iopub.execute_input":"2021-12-26T08:49:27.219920Z","iopub.status.idle":"2021-12-26T08:49:27.237818Z","shell.execute_reply.started":"2021-12-26T08:49:27.219892Z","shell.execute_reply":"2021-12-26T08:49:27.237244Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: One-hot encoding\n\nUse the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:27.238804Z","iopub.execute_input":"2021-12-26T08:49:27.239490Z","iopub.status.idle":"2021-12-26T08:49:27.286493Z","shell.execute_reply.started":"2021-12-26T08:49:27.239417Z","shell.execute_reply":"2021-12-26T08:49:27.285617Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach.","metadata":{}},{"cell_type":"code","source":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-12-26T08:49:27.287787Z","iopub.execute_input":"2021-12-26T08:49:27.288011Z","iopub.status.idle":"2021-12-26T08:49:29.213842Z","shell.execute_reply.started":"2021-12-26T08:49:27.287982Z","shell.execute_reply":"2021-12-26T08:49:29.212867Z"},"trusted":true},"execution_count":41,"outputs":[]}]}